

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Oct 16 22:00:57 2019

@author: pierre
"""


import os
if 'Notebooks' in os.getcwd(): os.chdir('..')  # change to main directory
print('Current directory: {}'.format( os.getcwd() ))

from cascade2p import checks
checks.check_packages()

import numpy as np
import matplotlib.pyplot as plt
import glob
import scipy.io as sio
# local folder
from cascade2p import cascade


def load_neurons_x_time(file_path):
    """Custom method to load data as 2d array with shape (neurons, nr_timepoints)"""
    
    # replace this with your own code if necessary
    # traces = np.load(file_path)
    
    # # here numpy dictionary with key 'dff'
#    traces = np.load(file_path, allow_pickle=True).item()['dff']
    
    # # In case your data is in another format:
    # traces = traces.T        # transpose, if loaded matrix has shape (time, neurons)
    # traces = traces / 100    # normalize to fractions, in case df/f is in Percent
    
    # traces should be 2d array with shape (neurons, nr_timepoints)
    
    traces = sio.loadmat(file_path)['dF_traces'] 
    
    return traces.T/100
  
  
example_file = 'Example_datasets/Multiplane-OGB1-zf-pDp-Rupprecht-7.5Hz/Calcium_traces_04.mat'

traces = load_neurons_x_time( example_file )

print('Number of neurons in dataset:', traces.shape[0])
print('Number of timepoints in dataset:', traces.shape[1])

# plot traces for some neurons in loaded dataset
np.random.seed(3952)
random_neurons = np.random.randint(traces.shape[0], size=10)
plt.figure(figsize=(8,7))
plt.subplot(2,1,1)

for i,neuron in enumerate(random_neurons):
    plt.plot(traces[neuron,:]+(i-1), alpha=0.8)
    
plt.xlabel('Timepoints')
plt.ylabel('Delta F/F (values normally between 0 and 4)')
plt.title('Random example traces')

# Zoom in
plt.subplot(2,1,2)
for neuron in random_neurons:
    plt.plot(traces[neuron,:], alpha=0.8)
    
plt.xlim(0, np.min((500, traces.shape[1])))
plt.title('Zoom in')
plt.xlabel('Timepoints')
plt.ylabel('Delta F/F (values normally between 0 and 4)')

plt.tight_layout()


model_name = 'OGB_pDp_7.5Hz'
traces_file_name = 'Example_datasets/Multiplane-OGB1-zf-pDp-Rupprecht-7.5Hz/Calcium_traces_04.mat'

traces = load_neurons_x_time( traces_file_name )


spike_rates = cascade.predict( model_name, traces )


folder = os.path.dirname(traces_file_name)
save_path = os.path.join(folder, 'full_prediction_'+os.path.basename(traces_file_name))

# save as numpy file
#np.save(save_path, spike_rates)
sio.savemat(save_path, {'spike_rates':spike_rates})

# save as .mat file
# import scipy
# scipy.io.savemat(save_path, {'spike_rates': spike_rates})

neuron = 5

plt.figure()

plt.plot(traces[neuron], label='Df/f')
plt.plot(spike_rates[neuron], label='Spike Rate')









"""

Fill up probabilities (output of the network) with discrete spikes

"""


## fileList is a list of mat-files with predictions
## fileList2 a list of mat-files with the corresponding calcium data
#fileList = glob.glob( os.path.join( test_dataset_folder, 'Predictions','Predictions_*.mat'))
#fileList2 = glob.glob( os.path.join( test_dataset_folder, 'Calcium*.mat'))
#
#for file,file2 in zip(fileList,fileList2):
#
#  prob_density_all = sio.loadmat(file)['Y_predict']
#  calcium_all = sio.loadmat(file2)['dF_traces']
#  
#  # initialize resulting list of spikes / matrix of approximations
#  # "approximations" show how well the inferred spikes match the input probabilities
#  # they are generated by convolving each inferred spike with the Gaussian kernel that
#  # was used for generating the ground truth
#  
#  spikes_all = []
#  approximations_all = np.nan*np.ones(prob_density_all.shape)
#  
#  for neuron in range(prob_density_all.shape[0]):
#    
#    print('Infer spikes for neuron '+str(neuron+1)+' out of '+str(prob_density_all.shape[0])+' for file '+basename(file2))
#    
#    prob_density = prob_density_all[neuron,:]
#    Calcium = calcium_all[:,neuron]/100
#    
#    spike_locs_all = []
#    
#    # find non-nan indices (first and last frames of predictions are NaNs)
#    nnan_indices = ~np.isnan(prob_density)
#    # offset in time to assign inferred spikes to correct positions in the end
#    offset = np.argmax(nnan_indices==True) - 1
#    
#    if np.sum(nnan_indices) > 0:
#    
#      prob_density = prob_density[nnan_indices]
#      Calcium = Calcium[nnan_indices]
#      
#      vector_of_indices = np.arange(0,len(prob_density))
#      # "support_slices", indices of continuous chunks of the array which are non-zero and which might contain spikes
#      support_slices = divide_and_conquer(prob_density,smoothing*sampling_rate)
#      
#      approximation = np.zeros(prob_density.shape)
#      # go through each slice separately
#      for k in range(len(support_slices)):
#        
#        spike_locs = []
#        
#        nb_spikes = np.sum(prob_density[support_slices[k]])
#        
#        # Monte Carlo/Metropolis-based sampling, initial guess of spikes
#        spike_locs,approximation[support_slices[k]],counter = fill_up_APs(prob_density[support_slices[k]],smoothing*sampling_rate,nb_spikes,spike_locs)
#        
#        # every spike is shifted to any other position (no sub-pixel resolution) and the best position is used
#        spike_locs,approximation[support_slices[k]] = systematic_exploration(prob_density[support_slices[k]],smoothing*sampling_rate,nb_spikes,spike_locs,approximation[support_slices[k]])
#
#        # refine initial guess using random shifts or removal of spikes
#        for jj in range(5):
#          # remove the worst spikes
#          spike_locs,approximation[support_slices[k]] = prune_APs(prob_density[support_slices[k]],smoothing*sampling_rate,nb_spikes,spike_locs,approximation[support_slices[k]])
#          # fill up spikes again
#          nb_spikes = np.sum(prob_density[support_slices[k]]) - np.sum(approximation[support_slices[k]])
#          spike_locs,approximation[support_slices[k]],counter = fill_up_APs(prob_density[support_slices[k]],smoothing*sampling_rate,nb_spikes,spike_locs)
#        
#      
#        temporal_offset = vector_of_indices[support_slices[k]][0]
#        new_spikes = spike_locs+temporal_offset
#        spike_locs_all.extend(new_spikes)
#        
#      approximations_all[neuron,nnan_indices] = approximation
#      
#    spikes_all.append(spike_locs_all+offset)
#  
#  # save results
#  stripped_path = os.path.basename(os.path.normpath(file))
#  sio.savemat(os.path.join(test_dataset_folder,'Predictions','Spikes_'+stripped_path),{'approximations_all':approximations_all,'spikes_all':spikes_all})
#
#
#
#time = np.arange(0,len(prob_density_all[0,:]))/sampling_rate
#
#index = [1,2,3,4]
#index = [59, 7, 8, 57]
#index = [11,12,13,14]
#fig, axs = plt.subplots(2,2)
#axs[0, 0].plot(time,prob_density_all[index[0],:]); axs[0, 0].plot(time,approximations_all[index[0],:]); axs[0, 0].set_ylim(-0.4, 3) 
#axs[0, 0].plot(time,calcium_all[:,index[0]]/100+1.5); 
#for spike in spikes_all[index[0]]:
#  axs[0,0].plot([spike/sampling_rate,spike/sampling_rate],[-0.2, -0.1],'k')
#axs[1, 0].plot(time,prob_density_all[index[1],:]); axs[1, 0].plot(time,approximations_all[index[1],:]); axs[1, 0].set_ylim(-0.4, 3) 
#axs[1, 0].plot(time,calcium_all[:,index[1]]/100+1.5); 
#for spike in spikes_all[index[1]]:
#  axs[1,0].plot([spike/sampling_rate,spike/sampling_rate],[-0.2, -0.1],'k')
#axs[0, 1].plot(time,prob_density_all[index[2],:]); axs[0, 1].plot(time,approximations_all[index[2],:]); axs[0, 1].set_ylim(-0.4, 3) 
#axs[0, 1].plot(time,calcium_all[:,index[2]]/100+1.5); 
#for spike in spikes_all[index[2]]:
#  axs[0,1].plot([spike/sampling_rate,spike/sampling_rate],[-0.2, -0.1],'k')
#axs[1, 1].plot(time,prob_density_all[index[3],:]); axs[1, 1].plot(time,approximations_all[index[3],:]); axs[1, 1].set_ylim(-0.4, 3) 
#axs[1, 1].plot(time,calcium_all[:,index[3]]/100+1.5); 
#for spike in spikes_all[index[3]]:
#  axs[1,1].plot([spike/sampling_rate,spike/sampling_rate],[-0.2, -0.1],'k')







